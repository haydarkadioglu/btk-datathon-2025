{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96d5adae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "İlk 5 satır:\n",
      "                  event_time event_type   product_id category_id      user_id  \\\n",
      "0  2025-06-19 10:23:07+00:00   ADD_CART  PROD_011223   CAT_00054  USER_097562   \n",
      "1  2025-06-07 21:34:45+00:00   ADD_CART  PROD_005519   CAT_00144  USER_006535   \n",
      "2  2025-06-21 21:29:09+00:00   ADD_CART  PROD_000577   CAT_00273  USER_047199   \n",
      "3  2025-06-09 09:10:20+00:00   ADD_CART  PROD_019235   CAT_00442  USER_082028   \n",
      "4  2025-06-19 11:13:58+00:00   ADD_CART  PROD_001702   CAT_00025  USER_096574   \n",
      "\n",
      "     user_session  session_value  \n",
      "0  SESSION_158779          90.29  \n",
      "1  SESSION_029987          16.39  \n",
      "2  SESSION_022134          64.27  \n",
      "3  SESSION_161308          41.67  \n",
      "4  SESSION_182859          86.11  \n",
      "\n",
      "Veri seti hakkında genel bilgiler:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 141219 entries, 0 to 141218\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Non-Null Count   Dtype  \n",
      "---  ------         --------------   -----  \n",
      " 0   event_time     141219 non-null  object \n",
      " 1   event_type     141219 non-null  object \n",
      " 2   product_id     141219 non-null  object \n",
      " 3   category_id    141219 non-null  object \n",
      " 4   user_id        141219 non-null  object \n",
      " 5   user_session   141219 non-null  object \n",
      " 6   session_value  141219 non-null  float64\n",
      "dtypes: float64(1), object(6)\n",
      "memory usage: 7.5+ MB\n",
      "\n",
      "Eksik veri sayıları:\n",
      "event_time       0\n",
      "event_type       0\n",
      "product_id       0\n",
      "category_id      0\n",
      "user_id          0\n",
      "user_session     0\n",
      "session_value    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('../../data/train.csv')\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(\"İlk 5 satır:\")\n",
    "print(df.head())\n",
    "\n",
    "# Get a summary of the dataframe\n",
    "print(\"\\nVeri seti hakkında genel bilgiler:\")\n",
    "df.info()\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nEksik veri sayıları:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c3a2469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yeni zaman özellikleri eklenmiş veri seti:\n",
      "                 event_time event_type   product_id category_id      user_id  \\\n",
      "0 2025-06-19 10:23:07+00:00   ADD_CART  PROD_011223   CAT_00054  USER_097562   \n",
      "1 2025-06-07 21:34:45+00:00   ADD_CART  PROD_005519   CAT_00144  USER_006535   \n",
      "2 2025-06-21 21:29:09+00:00   ADD_CART  PROD_000577   CAT_00273  USER_047199   \n",
      "3 2025-06-09 09:10:20+00:00   ADD_CART  PROD_019235   CAT_00442  USER_082028   \n",
      "4 2025-06-19 11:13:58+00:00   ADD_CART  PROD_001702   CAT_00025  USER_096574   \n",
      "\n",
      "     user_session  session_value  hour  day_of_week  month  \n",
      "0  SESSION_158779          90.29    10            3      6  \n",
      "1  SESSION_029987          16.39    21            5      6  \n",
      "2  SESSION_022134          64.27    21            5      6  \n",
      "3  SESSION_161308          41.67     9            0      6  \n",
      "4  SESSION_182859          86.11    11            3      6  \n",
      "\n",
      "'event_type' sütunu one-hot encoding ile dönüştürüldü:\n",
      "                 event_time event_type   product_id category_id      user_id  \\\n",
      "0 2025-06-19 10:23:07+00:00   ADD_CART  PROD_011223   CAT_00054  USER_097562   \n",
      "1 2025-06-07 21:34:45+00:00   ADD_CART  PROD_005519   CAT_00144  USER_006535   \n",
      "2 2025-06-21 21:29:09+00:00   ADD_CART  PROD_000577   CAT_00273  USER_047199   \n",
      "3 2025-06-09 09:10:20+00:00   ADD_CART  PROD_019235   CAT_00442  USER_082028   \n",
      "4 2025-06-19 11:13:58+00:00   ADD_CART  PROD_001702   CAT_00025  USER_096574   \n",
      "\n",
      "     user_session  session_value  hour  day_of_week  month  event_ADD_CART  \\\n",
      "0  SESSION_158779          90.29    10            3      6            True   \n",
      "1  SESSION_029987          16.39    21            5      6            True   \n",
      "2  SESSION_022134          64.27    21            5      6            True   \n",
      "3  SESSION_161308          41.67     9            0      6            True   \n",
      "4  SESSION_182859          86.11    11            3      6            True   \n",
      "\n",
      "   event_BUY  event_REMOVE_CART  event_VIEW  \n",
      "0      False              False       False  \n",
      "1      False              False       False  \n",
      "2      False              False       False  \n",
      "3      False              False       False  \n",
      "4      False              False       False  \n"
     ]
    }
   ],
   "source": [
    "# Convert 'event_time' to datetime objects\n",
    "df['event_time'] = pd.to_datetime(df['event_time'])\n",
    "\n",
    "# Extract time-based features\n",
    "df['hour'] = df['event_time'].dt.hour\n",
    "df['day_of_week'] = df['event_time'].dt.dayofweek # Monday=0, Sunday=6\n",
    "df['month'] = df['event_time'].dt.month\n",
    "\n",
    "# Display the dataframe with new features\n",
    "print(\"Yeni zaman özellikleri eklenmiş veri seti:\")\n",
    "print(df.head())\n",
    "\n",
    "# One-hot encode the 'event_type' column\n",
    "event_type_dummies = pd.get_dummies(df['event_type'], prefix='event')\n",
    "df = pd.concat([df, event_type_dummies], axis=1)\n",
    "\n",
    "print(\"\\n'event_type' sütunu one-hot encoding ile dönüştürüldü:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71158fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oturum bazında gruplanmış veri seti:\n",
      "                views  add_to_carts  removals_from_cart  buys  \\\n",
      "user_session                                                    \n",
      "SESSION_000000      0            20                   8     0   \n",
      "SESSION_000001      1             2                   2     1   \n",
      "SESSION_000004      1             0                   0     0   \n",
      "SESSION_000005      1             0                   0     0   \n",
      "SESSION_000012      1             0                   0     0   \n",
      "\n",
      "                unique_products  unique_categories  session_value  \\\n",
      "user_session                                                        \n",
      "SESSION_000000               24                 20         355.80   \n",
      "SESSION_000001                5                  5          96.60   \n",
      "SESSION_000004                1                  1          30.92   \n",
      "SESSION_000005                1                  1          40.09   \n",
      "SESSION_000012                1                  1          23.06   \n",
      "\n",
      "                total_events  \n",
      "user_session                  \n",
      "SESSION_000000            28  \n",
      "SESSION_000001             6  \n",
      "SESSION_000004             1  \n",
      "SESSION_000005             1  \n",
      "SESSION_000012             1  \n",
      "\n",
      "Aykırı değerleri temizlemeden önceki veri boyutu: (70736, 8)\n",
      "Aykırı değerleri temizledikten sonraki veri boyutu: (63921, 8)\n"
     ]
    }
   ],
   "source": [
    "# Aggregate data by user_session\n",
    "session_df = df.groupby('user_session').agg(\n",
    "    # Count the occurrences of each event type\n",
    "    views=('event_VIEW', 'sum'),\n",
    "    add_to_carts=('event_ADD_CART', 'sum'),\n",
    "    removals_from_cart=('event_REMOVE_CART', 'sum'),\n",
    "    buys=('event_BUY', 'sum'),\n",
    "    # Count unique products and categories\n",
    "    unique_products=('product_id', 'nunique'),\n",
    "    unique_categories=('category_id', 'nunique'),\n",
    "    # Take the mean of session_value\n",
    "    session_value=('session_value', 'mean'),\n",
    "    # Count total events in the session\n",
    "    total_events=('event_type', 'count')\n",
    ")\n",
    "\n",
    "print(\"Oturum bazında gruplanmış veri seti:\")\n",
    "print(session_df.head())\n",
    "\n",
    "# Outlier removal using the IQR method\n",
    "Q1 = session_df['session_value'].quantile(0.25)\n",
    "Q3 = session_df['session_value'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "print(f\"\\nAykırı değerleri temizlemeden önceki veri boyutu: {session_df.shape}\")\n",
    "\n",
    "session_df_no_outliers = session_df[(session_df['session_value'] >= lower_bound) & (session_df['session_value'] <= upper_bound)]\n",
    "\n",
    "print(f\"Aykırı değerleri temizledikten sonraki veri boyutu: {session_df_no_outliers.shape}\")\n",
    "\n",
    "# Save the processed data to a new CSV file\n",
    "session_df_no_outliers.to_csv('processed_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e047740",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:14: UserWarning: A NumPy version >=1.23.5 and <2.3.0 is required for this version of SciPy (detected version 2.3.2)\n",
      "  from scipy.sparse import csr_matrix, issparse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest modeli için Randomized Search başlatılıyor...\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:516: FitFailedWarning: \n",
      "30 fits failed out of a total of 60.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"f:\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"f:\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1358, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"f:\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 471, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"f:\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"f:\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"f:\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1358, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"f:\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 471, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"f:\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "f:\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1135: UserWarning: One or more of the test scores are non-finite: [0.43628082 0.43604786        nan 0.43358981        nan 0.43355953\n",
      "        nan 0.43791819        nan        nan        nan 0.42515149\n",
      " 0.43358064 0.43459163 0.4360209         nan 0.43578818        nan\n",
      "        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest için en iyi parametreler bulundu:\n",
      "{'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 10, 'bootstrap': True}\n",
      "\n",
      "LightGBM modeli için Randomized Search başlatılıyor...\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003593 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 115\n",
      "[LightGBM] [Info] Number of data points in the train set: 51136, number of used features: 7\n",
      "[LightGBM] [Info] Start training from score 30.726905\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "\n",
      "LightGBM için en iyi parametreler bulundu:\n",
      "{'subsample': 1.0, 'num_leaves': 31, 'n_estimators': 800, 'min_child_samples': 50, 'max_depth': 10, 'learning_rate': 0.01, 'colsample_bytree': 0.6}\n",
      "\n",
      "--- Model Değerlendirme Sonuçları ---\n",
      "\n",
      "Random Forest Performansı:\n",
      "  Ortalama Mutlak Hata (MAE): 10.5050\n",
      "  Ortalama Karesel Hata (MSE): 172.5527\n",
      "  Kök Ortalama Karesel Hata (RMSE): 13.1359\n",
      "  R-kare (R²): 0.4424\n",
      "\n",
      "LightGBM Performansı:\n",
      "  Ortalama Mutlak Hata (MAE): 10.5031\n",
      "  Ortalama Karesel Hata (MSE): 172.5887\n",
      "  Kök Ortalama Karesel Hata (RMSE): 13.1373\n",
      "  R-kare (R²): 0.4423\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Load the processed data\n",
    "data = pd.read_csv('processed_train.csv')\n",
    "\n",
    "# Prepare the data\n",
    "X = data.drop(['user_session', 'session_value'], axis=1)\n",
    "y = data['session_value']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- 1. Randomized Search for Random Forest ---\n",
    "print(\"Random Forest modeli için Randomized Search başlatılıyor...\")\n",
    "\n",
    "# Define the parameter grid\n",
    "rf_param_dist = {\n",
    "    'n_estimators': [int(x) for x in np.linspace(start=100, stop=1000, num=10)],\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'max_depth': [int(x) for x in np.linspace(10, 110, num=11)] + [None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Instantiate the model and RandomizedSearchCV\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "rf_random = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=rf_param_dist,\n",
    "    n_iter=20,  # Number of parameter settings that are sampled\n",
    "    cv=3,       # 3-fold cross-validation\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1   # Use all available cores\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nRandom Forest için en iyi parametreler bulundu:\")\n",
    "print(rf_random.best_params_)\n",
    "\n",
    "# --- 2. Randomized Search for LightGBM ---\n",
    "print(\"\\nLightGBM modeli için Randomized Search başlatılıyor...\")\n",
    "\n",
    "# Define the parameter grid\n",
    "lgb_param_dist = {\n",
    "    'n_estimators': [int(x) for x in np.linspace(start=100, stop=1000, num=10)],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'num_leaves': [20, 31, 40, 50, 60],\n",
    "    'max_depth': [-1, 10, 20, 30],\n",
    "    'min_child_samples': [20, 30, 50],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "}\n",
    "\n",
    "# Instantiate the model and RandomizedSearchCV\n",
    "lgbm = lgb.LGBMRegressor(random_state=42)\n",
    "lgb_random = RandomizedSearchCV(\n",
    "    estimator=lgbm,\n",
    "    param_distributions=lgb_param_dist,\n",
    "    n_iter=20,\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "lgb_random.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nLightGBM için en iyi parametreler bulundu:\")\n",
    "print(lgb_random.best_params_)\n",
    "\n",
    "# --- 3. Evaluate the best models ---\n",
    "print(\"\\n--- Model Değerlendirme Sonuçları ---\")\n",
    "\n",
    "# Get the best estimators\n",
    "best_rf = rf_random.best_estimator_\n",
    "best_lgb = lgb_random.best_estimator_\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = best_rf.predict(X_test)\n",
    "y_pred_lgb = best_lgb.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "def print_metrics(model_name, y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"\\n{model_name} Performansı:\")\n",
    "    print(f\"  Ortalama Mutlak Hata (MAE): {mae:.4f}\")\n",
    "    print(f\"  Ortalama Karesel Hata (MSE): {mse:.4f}\")\n",
    "    print(f\"  Kök Ortalama Karesel Hata (RMSE): {rmse:.4f}\")\n",
    "    print(f\"  R-kare (R²): {r2:.4f}\")\n",
    "\n",
    "print_metrics(\"Random Forest\", y_test, y_pred_rf)\n",
    "print_metrics(\"LightGBM\", y_test, y_pred_lgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33014409",
   "metadata": {},
   "source": [
    "## Testcsv pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bf7de09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Adım 3: Test Verisi Üzerinde Tahmin Yapılıyor ---\n",
      "Test verisi işlendi. Boyut: (30789, 7)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Adım 3: Test Verisi Üzerinde Tahmin Yapılıyor ---\")\n",
    "\n",
    "# Test verisini yükle\n",
    "try:\n",
    "    df_test = pd.read_csv('../../data/test.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Hata: 'test.csv' dosyası bulunamadı. Lütfen dosyanın doğru yolda olduğundan emin olun.\")\n",
    "    exit()\n",
    "\n",
    "# Eğitim verisine uygulanan adımların aynısını test verisine uygula\n",
    "df_test['event_time'] = pd.to_datetime(df_test['event_time'])\n",
    "df_test['hour'] = df_test['event_time'].dt.hour\n",
    "df_test['day_of_week'] = df_test['event_time'].dt.dayofweek\n",
    "df_test['month'] = df_test['event_time'].dt.month\n",
    "\n",
    "test_event_dummies = pd.get_dummies(df_test['event_type'], prefix='event')\n",
    "df_test = pd.concat([df_test, test_event_dummies], axis=1)\n",
    "\n",
    "# Test verisini user_session bazında grupla\n",
    "# Not: Test verisinde 'session_value' olmadığı için agg fonksiyonundan çıkarıldı\n",
    "test_session_df = df_test.groupby('user_session').agg(\n",
    "    views=('event_VIEW', 'sum'),\n",
    "    add_to_carts=('event_ADD_CART', 'sum'),\n",
    "    removals_from_cart=('event_REMOVE_CART', 'sum'),\n",
    "    buys=('event_BUY', 'sum'),\n",
    "    unique_products=('product_id', 'nunique'),\n",
    "    unique_categories=('category_id', 'nunique'),\n",
    "    total_events=('event_type', 'count')\n",
    ")\n",
    "\n",
    "# Eğitim ve test setlerindeki sütunları hizala\n",
    "training_columns = X_train.columns\n",
    "# Test setinde eksik olan sütunları bul ve 0 ile doldur\n",
    "missing_cols = set(training_columns) - set(test_session_df.columns)\n",
    "for c in missing_cols:\n",
    "    test_session_df[c] = 0\n",
    "# Sütunların sırasının eğitim setiyle aynı olduğundan emin ol\n",
    "test_session_df = test_session_df[training_columns]\n",
    "\n",
    "print(f\"Test verisi işlendi. Boyut: {test_session_df.shape}\")\n",
    "\n",
    "test_session_df.to_csv('processed_test.csv')\n",
    "# Tahminleri yap\n",
    "# test_predictions = best_lgb.predict(test_session_df)\n",
    "\n",
    "# # Submission dosyasını oluştur\n",
    "# submission_df = pd.DataFrame({\n",
    "#     'user_session': test_session_df.index,\n",
    "#     'session_value': test_predictions\n",
    "# })\n",
    "\n",
    "# # Olası negatif tahminleri 0'a çek\n",
    "# submission_df['session_value'] = submission_df['session_value'].clip(lower=0)\n",
    "\n",
    "# # Dosyayı kaydet\n",
    "# submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "# print(\"\\n'submission.csv' dosyası başarıyla oluşturuldu.\")\n",
    "# print(\"Submission dosyasının ilk 5 satırı:\")\n",
    "# print(submission_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5420250f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63584c02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f43608a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3265854b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec9fdc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd04bd7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
